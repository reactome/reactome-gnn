{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaca7d7",
   "metadata": {},
   "source": [
    "# Reactome_GNN Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30571e-2ba8-45d8-b574-292932741f24",
   "metadata": {},
   "source": [
    "This notebook shows how to obtain GNN-generated embeddings for nodes in the human pathway network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb67d38-533e-47ab-a913-9d235235d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import dgl\n",
    "from reactome_gnn import utils, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652ad38-cdf4-43f4-86f3-db4ea8d81500",
   "metadata": {},
   "source": [
    "## Embeddings of a single graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70dcc7-4625-4088-a63b-045f8f6e5383",
   "metadata": {},
   "source": [
    "Specify the list of markers and the threshold p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0507ada3-c0c0-4186-a30c-10fc31d3182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['RAS', 'MAP', 'IL10', 'EGF', 'EGFR', 'STAT']\n",
    "p_value = 0.05\n",
    "study = 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f8244-ebe7-4af2-ab03-8afa07bfe4d5",
   "metadata": {},
   "source": [
    "Create network from the markers and with specified p-value threshold, given the name of the study. This function first runs the enrichment analysis on the human pathway network and returns the significant nodes. This information is then used to create the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272a3ac7-c72d-432d-b4b2-0fb49aa0c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = utils.create_network_from_markers(markers, p_value, study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea89f1-a0c0-4a25-bb42-9f17e27002da",
   "metadata": {},
   "source": [
    "The main class attribute of the returned graph is `graph.graph_nx` which is a `NetworkX.DiGraph` object---a directed graph where each node has a weight and a significance. Weight is the p-value returned by the enrichment analysis, and significance is a string stating whether the pathway in the network is significant or not. Significant pathways are those that have p-value lower than the specified threshold, in this case 0.05.\n",
    "\n",
    "For example, let's take a look at the nodes corresponding to R-HSA-1358803 and R-HSA-15869 pathways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb5e873-1976-46aa-a28b-22339d2c349b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stId': 'R-HSA-1358803',\n",
       " 'name': 'Downregulation of ERBB2:ERBB3 signaling',\n",
       " 'weight': 0.0109,\n",
       " 'significance': 'significant'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph_nx.nodes['R-HSA-1358803']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196b4ec1-6fba-4625-947b-27c9b094a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stId': 'R-HSA-15869',\n",
       " 'name': 'Metabolism of nucleotides',\n",
       " 'weight': 0.161,\n",
       " 'significance': 'non-significant'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph_nx.nodes['R-HSA-15869']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01029b22-eac4-4d6e-8715-413df3ec93b8",
   "metadata": {},
   "source": [
    "We notice that R-HSA-1358803 corresponds to \"Downregulation of ERBB2:ERBB3 signaling\", and is significant in the network created with the markers specified above (its p-value is 0.0109 which is less than 0.05). On the other hand, R-HSA-15869 corresponds to \"Metabolism of nucleotides\" which is non-significant in this network (its p-value is 0.161)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63329ae-4520-4a50-9abe-b00fbe27e840",
   "metadata": {},
   "source": [
    "Next we want to do is create an embedding of that graph. But in order to do that, graph should be in the DGL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47e9b02-64c3-4d4c-bcbb-c0c42f1db087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=16050, num_edges=16850,\n",
       "      ndata_schemes={'weight': Scheme(shape=(), dtype=torch.float32), 'significance': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dgl = utils.nx_to_dgl(graph.graph_nx)\n",
    "graph_dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998e365-62bc-4a61-9707-5794d01aa556",
   "metadata": {},
   "source": [
    "We also need a GNN model which will produce the embeddings. Since we want the model to be reusable at any given time, the parameters of the model have to be saved to the disk and loaded upon model initialization. The demo model that is already saved is a two-layers Graph Convolutional Network (GCN) and creates the the embeddings with latent dimension of 8.\n",
    "\n",
    "The GCN is chosen because it is a relatively \"lightweight\" model that was proven to have good performance on various graph-related tasks. For example, even when comparing an untrained GCN network against DeepWalk on semi-supervised node classification task, untrained GCN had superior performance! This means that it can more successfully leverage the topology of a graph and create meaningful representation than the traditional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578d74f1-63f9-4113-a865-0ef0bcb7c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNModel(\n",
      "  (embedder): Embedding(2, 2)\n",
      "  (linear): Linear(in_features=3, out_features=8, bias=True)\n",
      "  (conv_0): GraphConv(in=8, out=8, normalization=both, activation=None)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=8, out=8, normalization=both, activation=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = model.GCNModel(dim_latent=8, num_layers=2)\n",
    "net.load_state_dict(torch.load('data/example/models/model.pth'))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae79414-2c92-475d-9312-6923dfb4de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16050, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "        [ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "        [ 0.0577,  0.3345, -0.0893,  ..., -0.2480, -0.3947, -0.4272],\n",
       "        ...,\n",
       "        [ 0.0763,  0.4480, -0.1193,  ..., -0.3276, -0.5198, -0.5664],\n",
       "        [ 0.0808,  0.4744, -0.1264,  ..., -0.3469, -0.5503, -0.5997],\n",
       "        [ 0.0777,  0.4559, -0.1215,  ..., -0.3334, -0.5289, -0.5764]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = net(graph_dgl).detach()\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca51a08-bdfe-47a1-9977-85539559d851",
   "metadata": {},
   "source": [
    "This tensor has a shape 16050 x 8, which means that each row represents the embedding of one node, as there are 16050 nodes in the pathway network, and each of these embeddings is 8-dimensional (as specified when initializing the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ed106-e22c-4930-8d00-eb56f59fb23e",
   "metadata": {},
   "source": [
    "To find the embeddings of some specific pathways, e.g. R-HSA-1358803, we have to load some auxilary information. We can also fetch embeddings of pathways by specifying their name, not only stId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac3033b-8d36-41c8-87c6-3ede4d6c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_id = pickle.load(open('data/example/info/name_to_id.pkl', 'rb'))\n",
    "sorted_stid_list = pickle.load(open('data/example/info/sorted_stid_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d593d8-048c-4855-9bb4-e5d3a8db4271",
   "metadata": {},
   "source": [
    "Here, `name_to_id` is a dictionary which maps each pathway name to its stId, and sorted_stid_list is, as the name states, the sorted list of all the stIds in the network. This is necessary due to how DGL stores the nodes when converting the network from the NetworkX format. Let's first obtain the embedding of R-HSA-1358803:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c20da87-392a-4f35-9869-e5b2b8dd8dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0194, -0.0138, -0.0101, -0.0111, -0.1170, -0.0788, -0.1274, -0.0852])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = sorted_stid_list.index('R-HSA-1358803')\n",
    "embeddings[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2aa1d-6b62-4ffd-a80c-fab33e482874",
   "metadata": {},
   "source": [
    "Let's now get the embedding of the pathway \"Metabolism of nucleotides\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f949f91c-44eb-41e8-a03a-432aa41a8a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0160,  0.0677, -0.0214, -0.0563, -0.1249, -0.0655, -0.1089, -0.1076])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stid = name_to_id['Metabolism of nucleotides']\n",
    "idx = sorted_stid_list.index(stid)\n",
    "embeddings[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38020b-6748-4cb4-810c-f2598e4e837a",
   "metadata": {},
   "source": [
    "## Working with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320242ea-919d-41be-a98a-2a695ab32b91",
   "metadata": {},
   "source": [
    "Steps above are useful when we are working with a single pathway network, but the procedure could be cumbersome when we need to work with several networks. That's where we can use datasets to automate part of the process. Here we will work with a toy dataset consisting of four different pathways, generated by lighting up pathways specified via their name. The networks are called A, B, C, and D, and the relations between them is such that $A = D$, $A \\& D \\subset C$, $B \\cap C = \\emptyset$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "205dcc46-a8a7-469a-be36-a6c387237832",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_A, graph_B, graph_c, graph_D = utils.create_toy_study_with_names(data_dir='data/example')\n",
    "embedding_dict = utils.create_embeddings(data_dir='data/example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d25ee-bb1c-488b-aac0-af45ffdadafe",
   "metadata": {},
   "source": [
    "What happens under the hood is that a PathwayDataset is created. This is a DGL dataset that makes processing raw graphs and loading DGL graphs easier. Creating this dataset is not mandatory, but helps when working with multiple graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95c12af-8581-4a29-a130-d06657f33ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_A.dgl\n",
      "Graph(num_nodes=16050, num_edges=16850,\n",
      "      ndata_schemes={'significance': Scheme(shape=(), dtype=torch.float32), 'weight': Scheme(shape=(), dtype=torch.float32)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "from reactome_gnn import dataset\n",
    "\n",
    "ds = dataset.PathwayDataset(root='data/example')\n",
    "\n",
    "graph, name = ds[0]\n",
    "print(name)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2755420-0736-45d7-bcf2-e24f8a0fb47e",
   "metadata": {},
   "source": [
    "But let's return to creating the embeddings. What's inside the embedding_dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef9f1f5-7def-41c7-9929-0a44b00aaf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'study_A.dgl': tensor([[ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0583,  0.3424, -0.0912,  ..., -0.2504, -0.3973, -0.4329],\n",
       "         ...,\n",
       "         [ 0.0763,  0.4480, -0.1193,  ..., -0.3276, -0.5198, -0.5664],\n",
       "         [ 0.0808,  0.4744, -0.1264,  ..., -0.3469, -0.5503, -0.5997],\n",
       "         [ 0.0777,  0.4559, -0.1215,  ..., -0.3334, -0.5289, -0.5764]]),\n",
       " 'study_B.dgl': tensor([[ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0583,  0.3424, -0.0912,  ..., -0.2504, -0.3973, -0.4329],\n",
       "         ...,\n",
       "         [ 0.0763,  0.4480, -0.1193,  ..., -0.3276, -0.5198, -0.5664],\n",
       "         [ 0.0808,  0.4744, -0.1264,  ..., -0.3469, -0.5503, -0.5997],\n",
       "         [ 0.0777,  0.4559, -0.1215,  ..., -0.3334, -0.5289, -0.5764]]),\n",
       " 'study_C.dgl': tensor([[ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0583,  0.3424, -0.0912,  ..., -0.2504, -0.3973, -0.4329],\n",
       "         ...,\n",
       "         [ 0.0763,  0.4480, -0.1193,  ..., -0.3276, -0.5198, -0.5664],\n",
       "         [ 0.0808,  0.4744, -0.1264,  ..., -0.3469, -0.5503, -0.5997],\n",
       "         [ 0.0777,  0.4559, -0.1215,  ..., -0.3334, -0.5289, -0.5764]]),\n",
       " 'study_D.dgl': tensor([[ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0569,  0.3340, -0.0890,  ..., -0.2442, -0.3875, -0.4222],\n",
       "         [ 0.0583,  0.3424, -0.0912,  ..., -0.2504, -0.3973, -0.4329],\n",
       "         ...,\n",
       "         [ 0.0763,  0.4480, -0.1193,  ..., -0.3276, -0.5198, -0.5664],\n",
       "         [ 0.0808,  0.4744, -0.1264,  ..., -0.3469, -0.5503, -0.5997],\n",
       "         [ 0.0777,  0.4559, -0.1215,  ..., -0.3334, -0.5289, -0.5764]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca468a7b-8e96-43b8-854e-7e494762b9bf",
   "metadata": {},
   "source": [
    "We see that we have obtained a dictionary, where keys are the names of the graphs, and the values are the embedding tensors for the corresponding graphs. We can obtain the embedding of each node in the same way as we did above, in the case of a single graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac737fb8-e99c-4277-95e1-30d4d8e4cd41",
   "metadata": {},
   "source": [
    "### Comparing the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c98590-e589-4167-835e-f9a8ded98037",
   "metadata": {},
   "source": [
    "To compare the embeddings obtained above, we use Canonical Correlation Analysis (CCA).  First we specify the targets Y for each graph, which specify which nodes are significant and which are not. Then we fit the data on the CCA model from scikit-learn. This fitted model can be used to transform embedding data and compare embeddings of nodes across different graphs. This is all done inside the `utils.fit_cca_on_toy_data()` function, so we can simply call it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f79d6668-92c3-46d1-af46-5039d1953f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca = utils.fit_cca_on_toy_data('data/example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2294fd-b3a3-4611-92b7-d4bea407657b",
   "metadata": {},
   "source": [
    "But for the sake of ilustration, we show here how it is done step-by-step so that the code can be reused with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6867c7a4-3abf-4514-b569-40d75ee650b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCA(n_components=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: We do not fit graph D, since it is equivalent to graph A\n",
    "\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "study_A = [\"Signaling by WNT\", \"WNT ligand biogenesis and trafficking\", \"Degradation of beta-catenin by the destruction complex\",\n",
    "           \"TCF dependent signaling in response to WNT\", \"Beta-catenin independent WNT signaling\"]\n",
    "study_B = [\"Autophagy\", \"Macroautophagy\", \"Chaperone Mediated Autophagy\", \"Late endosomal microautophagy\"]\n",
    "study_C = [\"Signal Transduction\", \"Signaling by NOTCH\", \"Signaling by NOTCH1\", \"Signaling by NOTCH2\", \"Signaling by NOTCH3\",\n",
    "           \"Signaling by NOTCH4\", \"Activated NOTCH1 Transmits Signal to the Nucleus\",\n",
    "           \"NOTCH1 Intracellular Domain Regulates Transcription\", \"Signaling by WNT\", \"WNT ligand biogenesis and trafficking\",\n",
    "           \"Degradation of beta-catenin by the destruction complex\", \"TCF dependent signaling in response to WNT\",\n",
    "           \"Beta-catenin independent WNT signaling\"]\n",
    "\n",
    "emb_A = embedding_dict['study_A.dgl']\n",
    "emb_B = embedding_dict['study_B.dgl']\n",
    "emb_C = embedding_dict['study_C.dgl']\n",
    "emb_D = embedding_dict['study_D.dgl']\n",
    "\n",
    "name_to_id = pickle.load(open('data/example/info/name_to_id.pkl', 'rb'))\n",
    "sorted_stid_list = pickle.load(open('data/example/info/sorted_stid_list.pkl', 'rb'))\n",
    "\n",
    "indices_A = [sorted_stid_list.index(id) for name, id in name_to_id.items() if name in study_A]\n",
    "indices_B = [sorted_stid_list.index(id) for name, id in name_to_id.items() if name in study_B]\n",
    "indices_C = [sorted_stid_list.index(id) for name, id in name_to_id.items() if name in study_C]\n",
    "\n",
    "y_A = torch.tensor([1.0 if i in indices_A else 0.0 for i in range(len(sorted_stid_list))]).unsqueeze(-1)\n",
    "y_B = torch.tensor([1.0 if i in indices_B else 0.0 for i in range(len(sorted_stid_list))]).unsqueeze(-1)\n",
    "y_C = torch.tensor([1.0 if i in indices_C else 0.0 for i in range(len(sorted_stid_list))]).unsqueeze(-1)\n",
    "\n",
    "cca = CCA(1)\n",
    "cca.fit(emb_A, y_A).fit(emb_B, y_B).fit(emb_C, y_C)\n",
    "cca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821c912-a762-43ed-b151-965e8336e465",
   "metadata": {},
   "source": [
    "Next thing we need to do is transform the embedding tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "719a2de2-ff7a-45fd-8bfc-b09f6d1892ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_A_t = cca.transform(emb_A)\n",
    "emb_B_t = cca.transform(emb_B)\n",
    "emb_C_t = cca.transform(emb_C)\n",
    "emb_D_t = cca.transform(emb_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8157b-2241-4710-b93e-6bac78d5c430",
   "metadata": {},
   "source": [
    "Finally, we can compare the obtained embeddings. For example, let's compare the embeddings of nodes that correspond to pathway \"WNT ligand biogenesis and trafficking\". This pathway is present in graphs A, C, and D, but not in B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6064930-1034-40d1-8443-9fb2fe1e0ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00019627] [-6.02214974e-07] [0.00028198] [0.00019627]\n"
     ]
    }
   ],
   "source": [
    "stid = name_to_id['WNT ligand biogenesis and trafficking']\n",
    "idx = sorted_stid_list.index(stid)\n",
    "\n",
    "print(emb_A_t[idx], emb_B_t[idx], emb_C_t[idx], emb_D_t[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262da5f-9b4b-4c02-8469-c648723e21a3",
   "metadata": {},
   "source": [
    "We can see that the values returned are identical in the case of A and D, as it should be since these networks are the same. We also notice that the values are relatively close to each other in case of A and C (as well as D and C), because this pathway is significant in all three networks. Finally, we notice that the value is significantly different (negative, even) in the case of network B in which this pathway is not significant. This shows that the embeddings are consistent within this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
